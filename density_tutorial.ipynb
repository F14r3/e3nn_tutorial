{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an E3NN network to predict electron densities\n",
    "\n",
    "In this tutorial we show how an E3NN network can be used to predict electron densities. One reason this might be a good idea is that electron densities can be represented in a spherical harmonic basis on atom centers. This fits naturally with the E3NN framework. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "from e3nn.kernel import Kernel\n",
    "from e3nn.point.operations import Convolution\n",
    "from e3nn.non_linearities import GatedBlock\n",
    "from e3nn.non_linearities import rescaled_act\n",
    "from e3nn.non_linearities.rescaled_act import relu, sigmoid\n",
    "from e3nn.radial import CosineBasisModel\n",
    "from e3nn.radial import GaussianRadialModel\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data. I have saved this in a pickle.\n",
    "For this particular example the dataset is ~6000 water dimer structures with density represented in \"a2\" density fitting basis set.\n",
    "\n",
    "Oxygen: 8s, 4p, 4d \n",
    "\n",
    "Hydrogen: 4s, 1p, 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load density data\n",
    "picklename = \"./density_data/dimer_data.pckl\"\n",
    "with open(picklename, 'rb') as f:\n",
    "    dataset_coeffs, dataset_onehot, dataset_geom, dataset_typemap, Rs_out_list, coeff_by_type = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define a model\n",
    "\n",
    "The thing that makes this task tricky is predicting different numbers of spherical harmonics on each atomic center. To address this problem, we introduce a class, Mixer, to handle this.\n",
    "\n",
    "Our network consists of 3 layers.\n",
    "1. Input layer: Geometry and one-hot atom encoding \n",
    "2. Middle layer: Rs = [(2,0),(1,1),(1,2)] (Up to L=2) Convolution + GatedBlock nonlinearity\n",
    "3. Final layer: 2 types of convolution, 1 for oxygen and 1 for hydrogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model\n",
    "\n",
    "class Mixer(torch.nn.Module):\n",
    "    def __init__(self, Op, Rs_in_s, Rs_out):\n",
    "        super().__init__()\n",
    "        self.ops = torch.nn.ModuleList([\n",
    "            Op(Rs_in, Rs_out)\n",
    "            for Rs_in in Rs_in_s\n",
    "        ])\n",
    "\n",
    "    def forward(self, *args, n_norm=1):\n",
    "        # It simply sums the different outputs\n",
    "        y = 0\n",
    "        for m, x in zip(self.ops, args):\n",
    "            y += m(*x, n_norm=n_norm)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, Rs_in, Rs_out_list, max_radius=3.0, number_of_basis=3, radial_layers=3, basistype=\"Gaussian\"):\n",
    "        super().__init__()\n",
    "\n",
    "        #sp = rescaled_act.Softplus(beta=5)\n",
    "        #sp = rescaled_act.ShiftedSoftplus(beta=5)\n",
    "        sp = torch.nn.Tanh()\n",
    "\n",
    "        # the [0] is just to get first_layer in stripped form.\n",
    "        # will not work for Rs_in with more than L=0\n",
    "        first_layer = Rs_in[0]\n",
    "        last_shared_layer = (2,1,1)\n",
    "\n",
    "        representations = [first_layer, last_shared_layer]\n",
    "        representations = [[(mul, l) for l, mul in enumerate(rs)] for rs in representations]\n",
    "\n",
    "        if (basistype == 'Gaussian'):\n",
    "            rad_basis = GaussianRadialModel\n",
    "        elif (basistype == 'Cosine'):\n",
    "            rad_basis = CosineBasisModel\n",
    "        else:\n",
    "            print (\"Only Gaussian and Cosine Radial basis are currently supported\")\n",
    "\n",
    "        RadialModel = partial(rad_basis, max_radius=max_radius,\n",
    "                              number_of_basis=number_of_basis, h=100,\n",
    "                              L=radial_layers, act=sp)\n",
    "\n",
    "        K = partial(Kernel, RadialModel=RadialModel)\n",
    "        C = partial(Convolution, K)\n",
    "        M = partial(Mixer, C)  # wrap C to accept many input types\n",
    "\n",
    "        def make_layer(Rs_in, Rs_out):\n",
    "            act = GatedBlock(Rs_out, sp, sigmoid)\n",
    "            conv = Convolution(K, Rs_in, act.Rs_in)\n",
    "            return torch.nn.ModuleList([conv, act])\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            make_layer(Rs_layer_in,Rs_layer_out)\n",
    "            for Rs_layer_in, Rs_layer_out in zip(representations[:-1], representations[1:])\n",
    "        ])\n",
    "\n",
    "        ## set up the split final layer\n",
    "        m = []\n",
    "        for rs in Rs_out_list:\n",
    "            m.append(M([representations[-1], representations[-1]], rs))\n",
    "        \n",
    "        # final layer is indexed in order of atom type\n",
    "        self.final_layer = torch.nn.ModuleList([\n",
    "            m[i] for i in range(len(m))\n",
    "        ])\n",
    "\n",
    "    def forward(self, input, geometry, atom_type_map):\n",
    "        output = input\n",
    "        batch, N, _ = geometry.shape\n",
    "\n",
    "        for conv, act in self.layers:\n",
    "            output = conv(output, geometry, n_norm=N)\n",
    "            output = act(output)\n",
    "\n",
    "        ## split final layer\n",
    "        geometry_list = []\n",
    "        feature_list = []\n",
    "        for i, item in enumerate(atom_type_map):\n",
    "            geometry_list.append(geometry[0][item])\n",
    "            feature_list.append(output[0][item])\n",
    "\n",
    "        ## this is assuming that there are only two atom types!\n",
    "        ## it should work, though for any arbitrary order of O and H in xyzfile!\n",
    "        featuresO = feature_list[0].unsqueeze(0)\n",
    "        featuresH = feature_list[1].unsqueeze(0)\n",
    "        geometryO = geometry_list[0].unsqueeze(0)\n",
    "        geometryH = geometry_list[1].unsqueeze(0)\n",
    "        \n",
    "        final_layer_output = []\n",
    "        for i, layer in enumerate(self.final_layer):\n",
    "            if (i == 0):\n",
    "                final = layer((featuresO, geometryO, geometryO), (featuresH, geometryH, geometryO), n_norm = N)\n",
    "            if (i == 1):\n",
    "                final = layer((featuresO, geometryO, geometryH), (featuresH, geometryH, geometryH), n_norm = N) \n",
    "            final_layer_output.append(final)\n",
    "\n",
    "        # return list of outputO and outputH\n",
    "        output = final_layer_output\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initilize th network\n",
    "\n",
    "Let's initialize a rough model. Here's a brief description of the parameters:\n",
    "- max_radius: Distance of the center of the farthest radial function from convolution center\n",
    "- num_basis: Number of radial basis functions to use; more=finer grain detail\n",
    "- radial_layers: Number of layers in radial basis network (number of nonlinearity operations)\n",
    "- basistype: What type of functions to use for radial basis; default is Gaussians\n",
    "\n",
    "We pass these parameters in as a dictionary so that we can save them for later use if we want to save the model.\n",
    "\n",
    "Then we send the model to the GPU\n",
    "\n",
    "The output shows us a helpful schematic of what kinds of operations our network is going to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (layers): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): Convolution(\n",
       "        (kernel): Kernel (2x0 -> 2x0,1,2,2x0)\n",
       "      )\n",
       "      (1): GatedBlock(\n",
       "        (scalar_act): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer): ModuleList(\n",
       "    (0): Mixer(\n",
       "      (ops): ModuleList(\n",
       "        (0): Convolution(\n",
       "          (kernel): Kernel (2x0,1,2 -> 8x0,4x1,4x2)\n",
       "        )\n",
       "        (1): Convolution(\n",
       "          (kernel): Kernel (2x0,1,2 -> 8x0,4x1,4x2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Mixer(\n",
       "      (ops): ModuleList(\n",
       "        (0): Convolution(\n",
       "          (kernel): Kernel (2x0,1,2 -> 4x0,1,2)\n",
       "        )\n",
       "        (1): Convolution(\n",
       "          (kernel): Kernel (2x0,1,2 -> 4x0,1,2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set arguments to network\n",
    "maxradius = 3.0\n",
    "numbasis = 20\n",
    "radiallayers = 3\n",
    "radialbasis = \"Gaussian\"\n",
    "## set Rs_in based on onehot vector\n",
    "Rs_in = [(len(dataset_typemap[0]),0)]\n",
    "mydict = {\"Rs_in\":Rs_in, \"Rs_out_list\":(Rs_out_list), \"max_radius\":maxradius,\n",
    "            \"number_of_basis\":numbasis, \"radial_layers\":radiallayers, \n",
    "            \"basistype\":radialbasis}\n",
    "\n",
    "net = Network(**mydict)\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set up training\n",
    "\n",
    "From here, training the model looks virtually identical to any other training one might do with a typical neural network in pytorch. In this case we are going to use the Adam optimizer and minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up training\n",
    "\n",
    "net.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "optimizer.zero_grad()\n",
    "loss_fn = torch.nn.modules.loss.MSELoss()\n",
    "\n",
    "max_steps = 2000\n",
    "minibatch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train and evaluate Test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0, Loss 0.11907389512059059\n",
      "\n",
      "Test Loss 0.11676265633713862\n",
      "\n",
      "Step 100, Loss 0.03316797714839528\n",
      "\n",
      "Test Loss 0.042259300544035296\n",
      "\n",
      "Step 200, Loss 0.006693016660111862\n",
      "\n",
      "Test Loss 0.03715066012161617\n",
      "\n",
      "Step 300, Loss 0.002286758580877273\n",
      "\n",
      "Test Loss 0.02890152602698776\n",
      "\n",
      "Step 400, Loss 0.000838089295964252\n",
      "\n",
      "Test Loss 0.03771684601099253\n",
      "\n",
      "Step 500, Loss 0.0013117253601525177\n",
      "\n",
      "Test Loss 0.03602544925479344\n",
      "\n",
      "Step 600, Loss 0.0005144868588334724\n",
      "\n",
      "Test Loss 0.03482248736254372\n",
      "\n",
      "Step 700, Loss 0.00041751631888181407\n",
      "\n",
      "Test Loss 0.034544754305458246\n",
      "\n",
      "Step 800, Loss 0.0003568325774010871\n",
      "\n",
      "Test Loss 0.03817526198501094\n",
      "\n",
      "Step 900, Loss 0.0001873451335784625\n",
      "\n",
      "Test Loss 0.03503706028378532\n",
      "\n",
      "Step 1000, Loss 6.141958215722778e-05\n",
      "\n",
      "Test Loss 0.03443993417625504\n",
      "\n",
      "Step 1100, Loss 0.00011824445958179925\n",
      "\n",
      "Test Loss 0.03423613068594088\n",
      "\n",
      "Step 1200, Loss 8.411632141934517e-05\n",
      "\n",
      "Test Loss 0.037264724659576864\n",
      "\n",
      "Step 1300, Loss 5.415781352633813e-05\n",
      "\n",
      "Test Loss 0.035049781645635346\n",
      "\n",
      "Step 1400, Loss 6.686695549640264e-05\n",
      "\n",
      "Test Loss 0.03619505101852396\n",
      "\n",
      "Step 1500, Loss 4.787284863498068e-05\n",
      "\n",
      "Test Loss 0.0368192071582819\n",
      "\n",
      "Step 1600, Loss 5.062631137662217e-05\n",
      "\n",
      "Test Loss 0.0343845242452389\n",
      "\n",
      "Step 1700, Loss 3.7928369445040146e-05\n",
      "\n",
      "Test Loss 0.03464841698806381\n",
      "\n",
      "Step 1800, Loss 3.775895585342251e-05\n",
      "\n",
      "Test Loss 0.0360050234588196\n",
      "\n",
      "Step 1900, Loss 4.688788001286915e-05\n",
      "\n",
      "Test Loss 0.03620078797615479\n"
     ]
    }
   ],
   "source": [
    "loss_minibatch = 0\n",
    "for step in range(max_steps):\n",
    "    i = random.randint(0, len(dataset_geom) - 3001)\n",
    "\n",
    "    onehot = dataset_onehot[i]\n",
    "    points = dataset_geom[i]\n",
    "    atom_type_map = dataset_typemap[i]\n",
    "    coeffs = dataset_coeffs[i]\n",
    "\n",
    "    outputO, outputH = net(onehot.to('cuda'),points.to('cuda'),atom_type_map)\n",
    "    outputO = torch.flatten(outputO)\n",
    "    outputH = torch.flatten(outputH)\n",
    "    output = torch.cat((outputO,outputH),0).view(1,1,-1)\n",
    "\n",
    "    loss = loss_fn(output.to('cuda'), coeffs.to('cuda'))\n",
    "    step_loss = loss.item()\n",
    "    loss.backward()\n",
    "    loss_minibatch += step_loss\n",
    "\n",
    "    if (step+1)%minibatch_size == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_minibatch = 0\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print('\\nStep {0}, Loss {1}'.format(step, step_loss))\n",
    "        j = random.randint(3000, len(dataset_geom) - 1)\n",
    "\n",
    "        onehot = dataset_onehot[j]\n",
    "        points = dataset_geom[j]*3\n",
    "        atom_type_map = dataset_typemap[j]\n",
    "        coeffs = dataset_coeffs[j]\n",
    "\n",
    "        outputO, outputH = net(onehot.to('cuda'),points.to('cuda'),atom_type_map)\n",
    "        outputO = torch.flatten(outputO)\n",
    "        outputH = torch.flatten(outputH)\n",
    "        output = torch.cat((outputO,outputH),0).view(1,1,-1)\n",
    "\n",
    "        loss = loss_fn(output.to('cuda'), coeffs.to('cuda'))\n",
    "        print('\\nTest Loss {0}'.format(loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now testing number of electrons on 10 randomly selected dimers\n",
      "\n",
      "Number of electrons for structure 4244:\n",
      "   True: 20.01397367619595\n",
      "     ML: 20.168971621791776\n",
      "\n",
      "Number of electrons for structure 5544:\n",
      "   True: 20.013671760120452\n",
      "     ML: 20.37076163987278\n",
      "\n",
      "Number of electrons for structure 4307:\n",
      "   True: 20.01374622715632\n",
      "     ML: 19.84391153206681\n",
      "\n",
      "Number of electrons for structure 5401:\n",
      "   True: 20.013734417529992\n",
      "     ML: 19.92265841669717\n",
      "\n",
      "Number of electrons for structure 4539:\n",
      "   True: 20.012783336427013\n",
      "     ML: 20.142424568600326\n",
      "\n",
      "Number of electrons for structure 5015:\n",
      "   True: 20.013881680878303\n",
      "     ML: 20.54106900971555\n",
      "\n",
      "Number of electrons for structure 3700:\n",
      "   True: 20.01415385491924\n",
      "     ML: 19.293529626836648\n",
      "\n",
      "Number of electrons for structure 4046:\n",
      "   True: 20.014699970000358\n",
      "     ML: 19.786969186866177\n",
      "\n",
      "Number of electrons for structure 4177:\n",
      "   True: 20.01361603500072\n",
      "     ML: 20.14281878417792\n",
      "\n",
      "Number of electrons for structure 3158:\n",
      "   True: 20.01407317479071\n",
      "     ML: 20.372974550255016\n"
     ]
    }
   ],
   "source": [
    "from density_analysis_utils import *\n",
    "\n",
    "testnumelectrons(net,10,\"./density_data/a2.gbs\",dataset_onehot,dataset_geom,dataset_typemap,coeff_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e3nn",
   "language": "python",
   "name": "e3nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
