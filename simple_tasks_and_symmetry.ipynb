{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Tasks and Symmetry\n",
    "### using the `e3nn` repository\n",
    "### tutorial by: Tess E. Smidt (`blondegeek`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are some unintuitive consequences of using E(3) equivariant neural networks. \n",
    "One example is that the symmetry your output has to be equal to or higher than the symmetry of your input. The following 3 simple tasks are to help demonstrate this:\n",
    "* Task 1: Distort a rectangle to a square.\n",
    "* Task 2: Distort a square to a rectangle.\n",
    "* Task 3: Distort a square to a rectangle -- with symmetry breaking (using representation theory).\n",
    "* Task 4: Distort a square to a rectangle -- with symmetry breaking (using gradients to change input).\n",
    "\n",
    "We will see that we can quickly do Task 1, but not Task 2. Only by using symmetry breaking in Task 3 and Task 4 are we able to distort a square into a rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import e3nn\n",
    "import e3nn.o3 as o3\n",
    "from e3nn.point.operations import Convolution\n",
    "from e3nn.non_linearities import GatedBlock\n",
    "from e3nn.kernel import Kernel\n",
    "from e3nn.kernel_mod import Kernel as KernelMod\n",
    "from e3nn.radial import CosineBasisModel\n",
    "from e3nn.non_linearities import rescaled_act\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from e3nn.spherical_tensor import SphericalTensor\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define out geometry\n",
    "square = torch.tensor(\n",
    "    [[0., 0., 0.], [1., 0., 0.], [1., 1., 0.], [0., 1., 0.]]\n",
    ")\n",
    "square -= square.mean(-2)\n",
    "sx, sy = 0.5, 1.5\n",
    "rectangle = square * torch.tensor([sx, sy, 0.])\n",
    "rectangle -= rectangle.mean(-2)\n",
    "\n",
    "N, _ = square.shape\n",
    "\n",
    "markersize = 15\n",
    "\n",
    "def plot_task(ax, start, finish, title, marker=None):\n",
    "    ax.plot(torch.cat([start[:, 0], start[:, 0]]), \n",
    "            torch.cat([start[:, 1], start[:, 1]]), 'o-', \n",
    "            markersize=markersize + 5 if marker else markersize, \n",
    "            marker=marker if marker else 'o')\n",
    "    ax.plot(torch.cat([finish[:, 0], finish[:, 0]]), \n",
    "            torch.cat([finish[:, 1], finish[:, 1]]), 'o-', markersize=markersize)\n",
    "    for i in range(N):\n",
    "        ax.arrow(start[i, 0], start[i, 1], \n",
    "                 finish[i, 0] - start[i, 0], \n",
    "                 finish[i, 1] - start[i, 1],\n",
    "                 length_includes_head=True, head_width=0.05, facecolor=\"black\", zorder=100)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(14, 6))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 6))\n",
    "plot_task(axes[0], rectangle, square, \"Task 1: Rectangle to Square\")\n",
    "plot_task(axes[1], square, rectangle, \"Task 2: Square to Rectangle\")\n",
    "# plot_task(axes[2], square, rectangle, \"Task 3: Square to Rectangle with Symmetry Breaking\", \"$\\u2B2E$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these tasks, we want to move 4 points in one configuration to another configuration. The input to the network will be the initial geometry and features on that geometry. The output will be used to signify \"displacement\" of each point to the new configuration. We can represent displacement in a couple different ways. The simplest way is to represent a displacement as an L=1 vector, `Rs=[(1, 1]]`. However, to better illustrate the symmetry properties of the network, we instead are going to use a spherical harmonic signal or more specifically, the peak of the spherical harmonic signal, to signify the displacement of the original point.\n",
    "\n",
    "First, we set up a very basic network that has the same representation list `Rs = [(1, L) for L in range(5 + 1)]` throughout the entire network. The input will be a spherical tensor with representation `Rs` and the output will also be a spherical tensor with representation `Rs`. We will interpret the output of the network as a spherical harmonic signal where the peak location will signify the desired displacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For these examples, we will used the default `e3nn.networks.GatedConvNetwork` class for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.networks import GatedConvNetwork\n",
    "L_max = 5\n",
    "Rs = [(1, L) for L in range(L_max + 1)]\n",
    "Network = partial(GatedConvNetwork, Rs_in=Rs, Rs_hidden=Rs, Rs_out=Rs, lmax=L_max, max_radius=3.0, kernel=KernelMod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Distort a rectangle to square.\n",
    "In this task, our input is a four points in the shape of a rectangle with simple scalars (1.0) at each point. The task is to learn to displace the points to form a (more symmetric) square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, 1e-2)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.zeros(1, N, sum(2 * L + 1 for L in range(L_max + 1)))\n",
    "input[:, :, 0] = 1.  # batch, point, channel\n",
    "\n",
    "displacements = square - rectangle\n",
    "N, _ = displacements.shape\n",
    "projections = torch.stack([SphericalTensor.from_geometry(displacements[i].unsqueeze(0), L_max).signal for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    output = model(input, rectangle.unsqueeze(0))\n",
    "    loss = loss_fn(output, projections.unsqueeze(0))\n",
    "    if i % 10 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spherical harmonic projections\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(start, finish, output, start_label, finish_label):\n",
    "    rows, cols = 1, 1\n",
    "    specs = [[{'is_3d': True} for i in range(cols)]\n",
    "             for j in range(rows)]\n",
    "    fig = make_subplots(rows=rows, cols=cols, specs=specs)\n",
    "    fig.add_trace(go.Scatter3d(x=start[:, 0], y=start[:, 1], z=start[:, 2], mode=\"markers\", name=start_label))\n",
    "    fig.add_trace(go.Scatter3d(x=finish[:, 0], y=finish[:, 1], z=finish[:, 2], mode=\"markers\", name=finish_label))\n",
    "    for i in range(N):\n",
    "        r, f = SphericalTensor(output[0][i].detach(), 1, L_max).plot(center=start[i])\n",
    "        trace = go.Surface(x=r[..., 0], y=r[..., 1], z=r[..., 2], surfacecolor=f.numpy())\n",
    "        trace.showscale = False\n",
    "        fig.add_trace(trace, 1, 1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input, rectangle.unsqueeze(0))\n",
    "fig = plot_output(rectangle, square, output, \"Rectangle\", \"Square\")\n",
    "fig.update_layout(scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And let's check that it's equivariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = torch.rand(3) * torch.tensor([np.pi, 2 * np.pi, np.pi])\n",
    "rot = o3.rot(*angles)\n",
    "rot_rectangle = torch.einsum('xy,jy->jx', (rot, rectangle))\n",
    "rot_square = torch.einsum('xy,jy->jx', (rot, square))\n",
    "output = model(input, rot_rectangle.unsqueeze(0))\n",
    "fig = plot_output(rot_rectangle, rot_square, output, \"Rectangle\", \"Square\")\n",
    "fig.update_layout(scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Now the reverse! Distort a square to rectangle.\n",
    "In this task, our input is a four points in the shape of a square with simple scalars (1.0) at each point. The task is to learn to displace the points to form a (less symmetric) rectangle. Can the network learn this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, 1e-2)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.zeros(1, N, sum(2 * L + 1 for L in range(L_max + 1)))\n",
    "input[:, :, 0] = 1.  # batch, point, channel\n",
    "\n",
    "displacements = rectangle - square\n",
    "N, _ = displacements.shape\n",
    "projections = torch.stack([SphericalTensor.from_geometry(displacements[i].unsqueeze(0), L_max).signal for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    output = model(input, square.unsqueeze(0))\n",
    "    loss = loss_fn(output, projections.unsqueeze(0))\n",
    "    if i % 10 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hmm... seems to get stuck. Let's try more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    output = model(input, square.unsqueeze(0))\n",
    "    loss = loss_fn(output, projections.unsqueeze(0))\n",
    "    if i % 10 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's stuck. What's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_output(square, rectangle, output, \"Square\", \"Rectangle\")\n",
    "fig.update_layout(scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The symmetry of the output must be higher or equal to the symmetry of the input! \n",
    "To be able to do this task, you need to give the network more information -- information that breaks the symmetry to that of the desired output. The square has a point group of $D_{4h}$ (16 elements) while the rectangle has a point group of $D_{2h}$ (8 elements).\n",
    "\n",
    "#### A technical note (for those who are interested).\n",
    "In this example, we are NOT using a network equivariant to [parity](https://en.wikipedia.org/wiki/Parity_(physics)) -- that will be in another update / tutorial -- so we are actually only sensitive to the fact that the square has $C_4$ symmetry while the rectangle has $C_2$ symmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Fixing Task 2. Distort a square into a rectangle -- now, with symmetry breaking (using representation theory)!\n",
    "\n",
    "In this task, our input is four points in the shape of a square with simple scalars (1.0) AND a contribution for the $x^2 - y^2$ feature at each point. The task is to learn to displace the points to form a (less symmetric) rectangle. Can the network learn this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, 1e-2)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.zeros(1, N, sum(2 * L + 1 for L in range(L_max + 1)))\n",
    "input[:, :, 0] = 1.  # batch, point, channel\n",
    "# Breaking x and y symmetry with x^2 - y^2 component\n",
    "input[:, :, 8] = 0.1  # x^2 - y^2\n",
    "\n",
    "displacements = rectangle - square\n",
    "N, _ = displacements.shape\n",
    "projections = torch.stack([SphericalTensor.from_geometry(displacements[i].unsqueeze(0), L_max).signal for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    output = model(input, square.unsqueeze(0))\n",
    "    loss = loss_fn(output, projections.unsqueeze(0))\n",
    "    if i % 10 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_output(square, rectangle, output, \"Square\", \"Rectangle\")\n",
    "fig.update_layout(scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is $x^2 - y^2$ the term doing? It's breaking the symmetry along the $\\hat{x}$ and $\\hat{y}$ directions.\n",
    "Notice how the shape below is an ellisoid elongated in the y direction and squished in the x. This isn't the only pertubation we could've added, but it is the most symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 1, 1\n",
    "specs = [[{'is_3d': True} for i in range(cols)]\n",
    "         for j in range(rows)]\n",
    "fig = make_subplots(rows=rows, cols=cols, specs=specs)\n",
    "\n",
    "L_max = 5\n",
    "Rs = [(1, L) for L in range(L_max + 1)]\n",
    "sum_Ls = sum(2 * L + 1 for mult, L in Rs) \n",
    "\n",
    "# Random spherical tensor up to L_Max\n",
    "signal = torch.zeros(sum_Ls)\n",
    "signal[0] = 1\n",
    "# Breaking x and y symmetry with x^2 - y^2\n",
    "signal[8] = -0.1\n",
    "\n",
    "sphten = SphericalTensor(signal, 1, L_max)\n",
    "\n",
    "r, f = sphten.plot(relu=False, n=60)\n",
    "trace = go.Surface(x=r[..., 0], y=r[..., 1], z=r[..., 2], surfacecolor=f.numpy())\n",
    "fig.add_trace(trace, row=1, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sure, but where did the $x^2 - y^2$ come from?\n",
    "It's a bit of a complicated story, but at the surface level here it is: [Character tables](https://en.wikipedia.org/wiki/Character_table) are handy tabulations of how certain spherical tensor datatypes transform under that group symmetry. The rows are irreducible representations (irrep for short) and the columns are similar elements of the group (called [conjugacy classes](https://en.wikipedia.org/wiki/Conjugacy_class)). Character tables are most commonly seen for finite groups of $E(3)$ symmetry as they are used extensively in solid state physics, crystallography, chemistry, etc. Next to the part of the table with the \"characters\", there are often columns showing linear, quadratic, and cubic functions (meaning they are of order 1, 2, and 3) that transform in the same way as a given irrep.\n",
    "\n",
    "So, a square has a point group symmetry of $D_{4h}$ while a rectangle has a point group symmetry of $D_{2h}$\n",
    "\n",
    "If we look at column headers of character tables for $D_{4h}$ and $D_{2h}$...\n",
    "* [$D_{4h}$ Character Table](http://symmetry.jacobs-university.de/cgi-bin/group.cgi?group=604&option=4)\n",
    "* [$D_{2h}$ Character Table](http://symmetry.jacobs-university.de/cgi-bin/group.cgi?group=602&option=4)\n",
    "\n",
    "... we can see that the irrep $B_{1g}$ of $D_{4h}$ that has -1's in the columns for all the symmetry operations that $D_{2h}$ DOESN'T have and if we look down that row to the column \"quadratic functions\" we see, voila $x^2 - y^2$. So, to break all those symmetries that $D_{4h}$ has that $D_{2h}$ DOESN'T have -- we add a non-zero contribution to the $x^2 - y^2$ component of our spherical harmonic tensors.\n",
    "\n",
    "#### WARNING: Character tables are written down with specific coordinate system conventions. For example, the $\\hat{z}$ axis always points along the highest symmetry axis, $\\hat{y}$ along the next highest, etc. We have specifically set up our problem have a coordinate frame that matches these conventions.\n",
    "\n",
    "#### A technical note (for those who are interested).\n",
    "Again, in this example (because we are choosing to leave out parity), we are only sensitive to the fact that the square has $C_4$ symmetry while the rectangle has $C_2$ symmetry. However, you can check the character tables for the point groups [$C_4$](http://symmetry.jacobs-university.de/cgi-bin/group.cgi?group=204&option=4) and [$C_2$](http://symmetry.jacobs-university.de/cgi-bin/group.cgi?group=202&option=4) to see that the arguement above still holds for the $x^2 - y^2$ order parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Fixing Task 2 without having to read character tables like Task 4. Distort a square into a rectangle -- now, with symmetry breaking (using gradients to change the input)!\n",
    "\n",
    "In this task, our input is four points in the shape of a square with simple scalars (1.0) AND then we LEARN how to change the inputs to break symmetry such that we can fit a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, 1e-2)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "input = torch.zeros(1, N, sum(2 * L + 1 for L in range(L_max + 1)))\n",
    "input[:, :, 0] = 1.  # batch, point, channel\n",
    "\n",
    "displacements = rectangle - square\n",
    "N, _ = displacements.shape\n",
    "projections = torch.stack([SphericalTensor.from_geometry(displacements[i].unsqueeze(0), L_max).signal for i in range(N)])\n",
    "input.requires_grad = True\n",
    "\n",
    "input_optimizer = torch.optim.Adam([input], 1e-3)\n",
    "input_loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacements = rectangle - square\n",
    "N, _ = displacements.shape\n",
    "projections = torch.stack([SphericalTensor.from_geometry(displacements[i].unsqueeze(0), L_max).signal for i in range(N)])\n",
    "projections = projections.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we'll train the model until it gets stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 201\n",
    "for i in range(iterations):\n",
    "    output = model(input, square.unsqueeze(0))\n",
    "    loss = loss_fn(output, projections)\n",
    "    if i % 30 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This gets stuck like before. So let's try alternating between updating our input and updating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 101\n",
    "eps = 1e-6\n",
    "for i in range(iterations):\n",
    "    output = model(input, square.unsqueeze(0))\n",
    "    loss = loss_fn(output, projections)\n",
    "    if i % 10 == 0:\n",
    "        print('model loss: ', loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    output = model(input, square.unsqueeze(0))\n",
    "    # This is the regular loss for the model\n",
    "    loss = input_loss_fn(output, projections)\n",
    "    # This is the loss for keeping the changes to the input small\n",
    "    loss += ((input[:, :, 0:1] - torch.ones_like(input[:, :, 0:1])).abs()).mean()\n",
    "    loss += ((input[:, :, 9:]).abs()).mean()\n",
    "    loss += ((input[:, :, 1:4]).abs()).mean()\n",
    "    # Prefer features on atoms to be the same (global parameter)\n",
    "    loss += ((input[:, :, 4:9] - input[:, 0, 4:9])**2).mean()\n",
    "    # and add a mild L1 penalty for the L=2 output.\n",
    "    loss += 1e-3 * ((input[:, :, 4:9]).abs()).mean()\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print('input loss: ', loss)\n",
    "    input_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    input_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we examine the input, we should see that the only components that are (largely) non-zero are the scalar features (which are all 1's) and the L=2 feature corresponding to $x^2 - y^2$, which is the 5th element of the L=2 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_decimal = 3\n",
    "print(\"L=0 \")\n",
    "print(input.detach().numpy().round(round_decimal)[:, :, 0])\n",
    "print(\"L=1\")\n",
    "print(input.detach().numpy().round(round_decimal)[:, :, 1: 1 + 3])\n",
    "print(\"L=2\")\n",
    "print(input.detach().numpy().round(round_decimal)[:, :, 4: 4 + 5])\n",
    "print(\"L=3\")\n",
    "print(input.detach().numpy().round(round_decimal)[:, :, 9: 9 + 7])\n",
    "print(\"L=4\")\n",
    "print(input.detach().numpy().round(round_decimal)[:, :, 16: 16 + 9])\n",
    "print(\"L=5\")\n",
    "print(input.detach().numpy().round(round_decimal)[:, :, 25: 25 + 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This plot shows what the new input looks like. It's similar to the above plot from Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_output(square, square, input, '', '')\n",
    "fig.update_layout(scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
